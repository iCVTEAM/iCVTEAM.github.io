<!DOCTYPE html>
<html>
  <head>
    <meta charset="utf-8">
    <title>E<sup>3</sup>NeRF</title>
    <link rel="stylesheet" type="text/css" href="assets/scripts/bulma.min.css">
    <link rel="stylesheet" type="text/css" href="assets/scripts/theme.css">
    <link rel="stylesheet" type="text/css" href="https://cdn.bootcdn.net/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  </head>
  <body>
    <section class="hero is-light" style="">
      <div class="hero-body" style="padding-top: 50px;">
        <div class="container" style="text-align: center;margin-bottom:5px;">
          <h1 class="title">
           <br> Seeing through Light and Darkness: Sensor-Physics Grounded Deblurring HDR<br>
            NeRF from Single-Exposure Images and Events </p>
          </h1>
          <div class="author"><p  style="font-size: 22px">
            Yunshan Qi<sup>1</sup> &nbsp&nbsp&nbsp&nbsp&nbsp Lin Zhu<sup>2 *</sup> &nbsp&nbsp&nbsp&nbsp&nbsp Nan Bao<sup>1</sup> &nbsp&nbsp&nbsp&nbsp&nbsp Yifan Zhao<sup>1</sup> &nbsp&nbsp&nbsp&nbsp&nbsp Jia Li<sup>1 *</sup></p></div>    
          <div class="aff">
            <p style="font-size: 20px"><sup>1</sup>Beihang University &nbsp&nbsp&nbsp&nbsp&nbsp <sup>2</sup>Beijing Institute of Technology
            <p> </p>
          </div>
          <div class="group"><p  style="font-size: 22px"><a href="http://cvteam.net/">CVTEAM</a></p></div>
          <div class="con">
            <p  style="font-size: 24px; margin-top:5px; margin-bottom: 15px;">
            <!--b>Accepted by International Conference on Computer Vision (ICCV) 2023</b-->
            </p>
          </div>
          <div class="columns">
            <div class="column"></div>
            <div class="column"></div>
            <!--div class="column">
              <a href="assets/E2NeRF/Papers.pdf" target="_blank">
                <p class="link"  style="font-size: 24px; margin-top:5px; margin-bottom: 15px;">[Paper]</p>
              </a>
            </-->
            <div class="column">
              <a href="https://arxiv.org/abs/2601.15475" target="_blank">
                <p class="link"  style="font-size: 24px; margin-top:5px; margin-bottom: 15px;">[Arxiv]</p>
              </a>
            </div>
            <div class="column">
              <a href="https://github.com/iCVTEAM/See-NeRF" target="_blank">
                <p class="link"  style="font-size: 24px; margin-top:5px; margin-bottom: 15px;">[Code]</p>
              </a>
            </div>
            <div class="column">
              <a href="https://github.com/iCVTEAM/See-NeRF" target="_blank">
                <p class="link"  style="font-size: 24px; margin-top:5px; margin-bottom: 15px;">[Dataset]</p>
              </a>
            </div>
            <div class="column"></div>
            <div class="column"></div>
          </div>
        </div>
      </div>
    </section>


    <div style="text-align: center;">
      <div class="head_cap">
        <p style="font-size: 20px">
          <b>The Motivation of See-NeRF</b>
        </p>
      </div>
      <div class="container" style="max-width:500px">
        <div style="text-align: center;">
          <img src="./assets/See-NeRF/motivation.png" class="centerImage">
        </div>
      </div>
    </div>

    <section class="hero">
      <div class="hero-body">
        <div class="container" style="max-width: 800px" >
          <h1 style="">Abstract</h1>
          <p  style="text-align: justify; font-size: 16px;">
            Novel view synthesis from low dynamic range (LDR) blurry images, which are common in the wild, struggles to recover high dynamic range (HDR) and sharp 3D representations in extreme lighting conditions. 
            Although existing methods employ event data to address this issue, they ignore the sensor-physics mismatches between the camera output and physical world radiance, resulting in suboptimal HDR and deblurring results. 
            To cope with this problem, we propose a unified sensor-physics grounded NeRF framework for sharp HDR novel view synthesis from single-exposure blurry LDR images and corresponding events. 
            We employ NeRF to directly represent the actual radiance of the 3D scene in the HDR domain and model raw HDR scene rays hitting the sensor pixels as in the physical world. 
            A pixel-wise RGB mapping field is introduced to align the above rendered pixel values with the sensor-recorded LDR pixel values of the input images. 
            A novel event mapping field is also designed to bridge the physical scene dynamics and actual event sensor output. 
            The two mapping fields are jointly optimized with the NeRF network, leveraging the spatial and temporal dynamic information in events to enhance the sharp HDR 3D representation learning. 
            Experiments on the collected and public datasets demonstrate that our method can achieve state-of-the-art deblurring HDR novel view synthesis results with single-exposure blurry LDR images and corresponding events.
          </p>
        </div>
      </div>
    </section>

    <div style="text-align: center;">
      <div class="head_cap">
        <p style="font-size: 20px">
          <b>The Framework of See-NeRF</b>
        </p>
      </div>
      <div class="container" style="max-width:850px">
        <div style="text-align: center;">
          <img src="./assets/See-NeRF/method.png" class="centerImage">
        </div>
      </div>
    </div>

    
    <!--div style="text-align: center;">
      <div class="head_cap">
        <p style="font-size: 20px">
          <b>The Results of E<sup>3</sup>NeRF</b>
        </p>
      </div>
      <div class="container" style="max-width:850px">
        <div style="text-align: center;">
          <img src="./assets/E3NeRF/gif.gif" class="centerImage">
        </div>
      </div>
    </div-->

    <!--div style="text-align: center;">
      <div class="head_cap">
        <p style="font-size: 26px">
          <b>Video Results of Real-World-Challenge Dataset
        </p>
      </div>
      <div class="container" style="max-width:850px">
        <div style="text-align: center;">
          <iframe width="800" height="350" src="https://www.youtube.com/embed/yz4xdOx90VY?si=v0H0oMSTsvlh3Rb1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>
        </div>
      </div>
      <div class="head_cap">
        <p style="font-size: 26px">
          <b>Video Results of Synthetic Severely Shaking Dataset
        </p>
      </div>
      <div class="container" style="max-width:850px">
        <div style="text-align: center;">
          <iframe width="800" height="450" src="https://www.youtube.com/embed/GWOp_ENikMw?si=upDrYUbEtji-khdj" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
        </div>
      </div>
    </div-->






    <!--section class="hero is-light" style="background-color:#FFFFFF;">
      <div class="hero-body">
        <div class="container" style="max-width:800px;margin-bottom:20px;">
          <h1>
            Qualitative Comparison on Synthetic Data
          </h1>
        </div>
        <div class="container" style="max-width:800px">
          <div style="text-align: center;">
            <img src="./assets/E2NeRF/qualitative1.png" class="centerImage">
          </div>
        </div>
        <div class="container" style="max-width:800px;margin-bottom:20px;">
          <h1>
            Qualitative Comparison on Real Data
          </h1>
        </div>
        <div class="container" style="max-width:800px">
          <div style="text-align: center;">
            <img src="./assets/E2NeRF/qualitative2.png" class="centerImage">
          </div>
        </div>
      </div>
    </section>

  <section class="hero" style="padding-top:0px;">
    <div class="hero-body">
      <div class="container" style="max-width:800px;">
  <div class="card"-->
  <!--header class="card-header">
    <p class="card-header-title">
      BibTex Citation
    </p>

    <a class="card-header-icon button-clipboard" style="border:0px; background: inherit;" data-clipboard-target="#bibtex-info" >
      <i class="fa fa-copy" height="20px"></i>
    </a>
  </header>
    <div class="card-content">
<pre style="background-color:inherit;padding: 0px;" id="bibtex-info">@article{Ma_Xia_Li_2021,
  title={Pyramidal Feature Shrinking for Salient Object Detection},
  volume={35},
  url={https://ojs.aaai.org/index.php/AAAI/article/view/16331},
  number={3},
  journal={Proceedings of the AAAI Conference on Artificial Intelligence},
  author={Ma, Mingcan and Xia, Changqun and Li, Jia},
  year={2021},
  month={May},
  pages={2311-2318}
}</pre-->
    </div>
    </section>
    <script type="text/javascript" src="assets/scripts/clipboard.min.js"></script>
    <script>
      new ClipboardJS('.button-clipboard');
    </script>
  </body>
</html>
